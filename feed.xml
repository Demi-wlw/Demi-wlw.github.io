<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://demi-wlw.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://demi-wlw.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-10T23:10:46+00:00</updated><id>https://demi-wlw.github.io/feed.xml</id><title type="html">blank</title><subtitle>Luwei Wang&apos;s personal homepage. </subtitle><entry><title type="html">tmux: a fancy terminal tool for running jobs on compute server</title><link href="https://demi-wlw.github.io/blog/2024/tmux/" rel="alternate" type="text/html" title="tmux: a fancy terminal tool for running jobs on compute server"/><published>2024-06-20T00:00:00+00:00</published><updated>2024-06-20T00:00:00+00:00</updated><id>https://demi-wlw.github.io/blog/2024/tmux</id><content type="html" xml:base="https://demi-wlw.github.io/blog/2024/tmux/"><![CDATA[]]></content><author><name></name></author><category term="external-posts"/><category term="code"/><summary type="html"><![CDATA[Redirecting to a website]]></summary></entry><entry><title type="html">Summary of R commands</title><link href="https://demi-wlw.github.io/blog/2024/R-cheatsheet/" rel="alternate" type="text/html" title="Summary of R commands"/><published>2024-02-10T00:00:00+00:00</published><updated>2024-02-10T00:00:00+00:00</updated><id>https://demi-wlw.github.io/blog/2024/R-cheatsheet</id><content type="html" xml:base="https://demi-wlw.github.io/blog/2024/R-cheatsheet/"><![CDATA[]]></content><author><name></name></author><category term="site-posts"/><category term="code"/><summary type="html"><![CDATA[Redirecting to a pdf]]></summary></entry><entry><title type="html">Installing R in Conda Env and easy use with VScode</title><link href="https://demi-wlw.github.io/blog/2024/Conda-R4vscode/" rel="alternate" type="text/html" title="Installing R in Conda Env and easy use with VScode"/><published>2024-01-28T00:00:00+00:00</published><updated>2024-01-28T00:00:00+00:00</updated><id>https://demi-wlw.github.io/blog/2024/Conda-R4vscode</id><content type="html" xml:base="https://demi-wlw.github.io/blog/2024/Conda-R4vscode/"><![CDATA[<p>There is already a useful <a href="https://astrobiomike.github.io/R/managing-r-and-rstudio-with-conda">Webpage</a> describing general steps. However, I would prefer to summarize it below with an easy way to use in VScode.</p> <h2 id="setting-up-a-conda-environment-with-r"><strong>Setting up a Conda Environment with R</strong></h2> <p>Can either install R by creating a new conda environment</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">-n</span> ENVNAME <span class="nt">-c</span> conda-forge r-base<span class="o">=</span>R-VERSION
</code></pre></div></div> <p>or install R directly within an activated conda environment</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge r-base<span class="o">=</span>R-VERSION
</code></pre></div></div> <h2 id="installing-essential-packages-for-vscode-r"><strong>Installing Essential Packages for VScode-R</strong></h2> <p class="text-justify">The R Essentials bundle contains approximately 200 of the most popular R packages for data science, including the <code class="language-plaintext highlighter-rouge">IRKernel</code>, <code class="language-plaintext highlighter-rouge">dplyr</code>, <code class="language-plaintext highlighter-rouge">shiny</code>, <code class="language-plaintext highlighter-rouge">ggplot2</code>, <code class="language-plaintext highlighter-rouge">tidyr</code>, <code class="language-plaintext highlighter-rouge">caret</code>, and <code class="language-plaintext highlighter-rouge">nnet</code>. It is good enough to install this package for using R with conda env. Apart from that, <code class="language-plaintext highlighter-rouge">httpgd</code>, <code class="language-plaintext highlighter-rouge">languageserver</code> and <code class="language-plaintext highlighter-rouge">radian</code> are also three useful packages for <strong>VScode-R</strong> as stated in the R extension info. To quickly install them together with R Essentials, I uploaded a package bundle <code class="language-plaintext highlighter-rouge">r-4vscode</code> for easy installation:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> demiwlw r-4vscode
</code></pre></div></div> <p>Now you can simply execute <code class="language-plaintext highlighter-rouge">radian</code> (or <code class="language-plaintext highlighter-rouge">r</code> if you add an alias for <code class="language-plaintext highlighter-rouge">radian</code>) in the terminal to open the R console in the activated conda environment!</p> <blockquote class="block-tip"> <h5 id="tips">TIPS</h5> <ol> <li>To set an alias for radian, execute (for MacOS)<br/> <code class="language-plaintext highlighter-rouge">echo 'alias r="radian"' &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc</code></li> <li>To create a simple custom bundle metapackage that contains several popular programs and their dependencies:<br/> <code class="language-plaintext highlighter-rouge">conda metapackage custom-bundle 0.1.0 --dependencies PACKAGES --summary "My custom bundle"</code><br/> Then we can share the new metapackage by uploading it to the channel on <a href="https://anaconda.org/">anaconda.org</a>.</li> </ol> </blockquote>]]></content><author><name></name></author><category term="site-posts"/><category term="code"/><summary type="html"><![CDATA[A better way to control the R version for different projects and easy use with the VScode console]]></summary></entry><entry><title type="html">Summary of Conda commands</title><link href="https://demi-wlw.github.io/blog/2023/Conda-cheatsheet/" rel="alternate" type="text/html" title="Summary of Conda commands"/><published>2023-11-10T00:00:00+00:00</published><updated>2023-11-10T00:00:00+00:00</updated><id>https://demi-wlw.github.io/blog/2023/Conda-cheatsheet</id><content type="html" xml:base="https://demi-wlw.github.io/blog/2023/Conda-cheatsheet/"><![CDATA[]]></content><author><name></name></author><category term="site-posts"/><category term="code"/><summary type="html"><![CDATA[Redirecting to a pdf]]></summary></entry><entry><title type="html">How to use Eddie: the ECDF Linux compute cluster</title><link href="https://demi-wlw.github.io/blog/2023/Eddie/" rel="alternate" type="text/html" title="How to use Eddie: the ECDF Linux compute cluster"/><published>2023-09-25T00:00:00+00:00</published><updated>2023-09-25T00:00:00+00:00</updated><id>https://demi-wlw.github.io/blog/2023/Eddie</id><content type="html" xml:base="https://demi-wlw.github.io/blog/2023/Eddie/"><![CDATA[<p>More about Eddie please see <a href="https://www.ed.ac.uk/information-services/research-support/research-computing/ecdf/high-performance-computing">here</a>. Its command is based on the <a href="https://gridscheduler.sourceforge.net/htmlman/manuals.html">Grid Scheduler</a>.</p> <h2 id="login"><strong>Login</strong></h2> <p>Open the terminal and input:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh &lt;YOUR UUN&gt;@eddie.ecdf.ed.ac.uk
</code></pre></div></div> <p>to login to the home directory.</p> <blockquote class="block-warning"> <h5 id="warning">WARNING</h5> <p>As mentioned by the IT staff, Eddie of UoE does not support the <code class="language-plaintext highlighter-rouge">Remote-ssh</code> login in VScode extension due to security issues. But it is fine to use the <code class="language-plaintext highlighter-rouge">scp</code> or <code class="language-plaintext highlighter-rouge">sftp</code> extension to only manage the file system.</p> </blockquote> <h2 id="folder-path"><strong>Folder Path</strong></h2> <p>This is the shared group space in UoE. <br/> <strong>Eddie</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/exports/&lt;COLLEGE&gt;/eddie/&lt;SCHOOL&gt;/groups/&lt;GROUP NAME&gt;/
</code></pre></div></div> <p><strong>DataStore</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/exports/&lt;COLLEGE&gt;/datastore/&lt;SCHOOL&gt;/groups/&lt;GROUP NAME&gt;/
</code></pre></div></div> <blockquote class="block-tip"> <h5 id="tip">TIP</h5> <p>All-caps words in <code class="language-plaintext highlighter-rouge">&lt; &gt;</code> should be replaced with your own choices. DataStore is a network file storage service.</p> </blockquote> <h2 id="basic-commands-on-eddie"><strong>Basic Commands on Eddie</strong></h2> <p>To start an interactive session run:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qlogin
<span class="c"># The default maximum runtime is 48 hours. The default resources: 1 core, 1 GB memory.</span>

qlogin <span class="nt">-l</span> <span class="nv">h_vmem</span><span class="o">=</span>1G
<span class="c"># request memory for an interactive session</span>

qlogin <span class="nt">-pe</span> gpu 1
<span class="c"># request 1 gpu (need for torch and memory usage is at least 2GB)</span>

qlogin <span class="nt">-q</span> staging
<span class="c"># choose the staging environment</span>
</code></pre></div></div> <p>Staging data (transfer data between Eddie and DataStore):</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qsub stagein/out.sh
<span class="c"># stagein/out.sh file content can be seen in Eddie handbook (need UUN login)</span>
</code></pre></div></div> <blockquote class="block-tip"> <h5 id="tip-1">TIP</h5> <p>It is also possible to sync the files between local computer and Eddie directly using <code class="language-plaintext highlighter-rouge">scp</code> or <code class="language-plaintext highlighter-rouge">sftp</code>.</p> </blockquote> <p>To view current running jobs</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qstat
</code></pre></div></div> <p>To cancel a job,</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qdel &lt;JOB IDENTIFIER&gt;
</code></pre></div></div> <p>Detailed information about completed jobs is available with the qacct command:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qacct <span class="nt">-j</span> &lt;JOB IDENTIFIER&gt;
</code></pre></div></div> <h2 id="set-up-the-anaconda-environment"><strong>Set up the Anaconda Environment</strong></h2> <h3 id="add-path-and-create-a-virtual-environment">Add path and create a virtual environment</h3> <p class="text-justify">Configure the path for your environments directory, i.e., the directory where all your conda environments will be stored, and packages (pkgs) directory used as a cache for your conda packages. By default cache for packages will be placed under <code class="language-plaintext highlighter-rouge">~/.conda/pkgs</code> and will very likely cause your home directory to go over quota as a result. We recommend using the group space you have used for your conda environments instead. <strong>Note that this should be an existing directory, so you need to create it first if it doesn’t already exist, before running the command below.</strong></p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load anaconda <span class="c"># version 5.0.1</span>
</code></pre></div></div> <p>Skip this step if already created the virtual environment</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda config <span class="nt">--add</span> envs_dirs /exports/&lt;COLLEGE&gt;/eddie/&lt;SCHOOL&gt;/groups/&lt;GROUP NAME&gt;/anaconda/envs

conda config <span class="nt">--add</span> pkgs_dirs /exports/&lt;COLLEGE&gt;/eddie/&lt;SCHOOL&gt;/groups/&lt;GROUP NAME&gt;/anaconda/pkgs
</code></pre></div></div> <p>Once you have configured your <code class="language-plaintext highlighter-rouge">envs_dirs</code> and <code class="language-plaintext highlighter-rouge">pkgs_dirs</code>, you can create an environment. The example below creates an environment specifying particular Python versions (also the version used for this project):</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">-n</span> %env_name% <span class="nv">python</span><span class="o">=</span>3.9
</code></pre></div></div> <p>To list your conda environments:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda info <span class="nt">--envs</span>
</code></pre></div></div> <h3 id="activate-your-environment">Activate your environment</h3> <p>Switch to the created environment and deactivate the environment when no longer needed:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>activate %env_name%
<span class="nb">source </span>deactivate
</code></pre></div></div> <h3 id="install-required-packages">Install required packages</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span>pip
pip <span class="nt">--no-cache-dir</span> <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
<span class="c"># `--no-cache-dir` is to ensure torch installed successfully</span>
</code></pre></div></div> <p><strong>Some package management commands:</strong></p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda list
<span class="c"># To see what packages are in the active environment</span>
conda search matplotlib
<span class="c"># To search for a package in the Anaconda repositories</span>
conda <span class="nb">install </span><span class="nv">matplotlib</span><span class="o">=</span>1.4.1
<span class="c"># To install a package e.g. matplotlib version 1.4.1</span>
</code></pre></div></div> <h4 id="using-pip">Using pip</h4> <p class="text-justify">If a package is not available in the Anaconda repositories it can still be installed with <code class="language-plaintext highlighter-rouge">pip</code> in the usual way. First, install <code class="language-plaintext highlighter-rouge">pip</code> into your active Anaconda environment. You must install pip into your environment first before running a pip command. Otherwise, <code class="language-plaintext highlighter-rouge">pip install</code> will use the pip installed in the Anaconda directory (which users do not have write access to) and your installation will fail. Once you have installed pip in your environment, then you can install a package using pip into your environment:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span>pip
pip <span class="nb">install </span>matplotlib
</code></pre></div></div> <h2 id="set-up-r-environment"><strong>Set up R Environment</strong></h2> <h3 id="configure-r-packages-path">Configure R packages path</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load cmake/3.5.2
<span class="c"># some packages may need this to be loaded first</span>
module load igmm/apps/R/4.1.3
<span class="nb">export </span><span class="nv">R_LIBS</span><span class="o">=</span>/exports/&lt;COLLEGE&gt;/eddie/&lt;SCHOOL&gt;/groups/&lt;GROUP NAME&gt;/rlibs
</code></pre></div></div> <p><strong>Install required packages in R console</strong></p> <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">install.packages</span><span class="p">(</span><span class="s2">"tidyverse"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <h2 id="run-code-on-eddie"><strong>Run Code on Eddie</strong></h2> <h3 id="through-job-submission">Through job submission</h3> <p><strong>Example</strong></p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qsub run_job.sh
<span class="c"># job submission files are available in shell folder</span>
</code></pre></div></div> <h3 id="through-interaction-session">Through interaction session</h3> <p>First start an interaction session, specify max run time, memory usage (gpu) and activate the environment then run according to the following:</p> <p><strong>Run Jupyter notebook</strong></p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter nbconvert <span class="nt">--to</span> notebook <span class="nt">--inplace</span> <span class="nt">--execute</span> jupyter-notebook.ipynb
</code></pre></div></div> <p>Note that <code class="language-plaintext highlighter-rouge">nbconvert</code> will fail if any cell takes longer than 30s to run, then add the following option to disable timing</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--ExecutePreprocessor.timeout=-1
</code></pre></div></div> <p>View jupyter notebook ouptputs</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nbpreview jupyter-notebook.ipynb
</code></pre></div></div>]]></content><author><name></name></author><category term="site-posts"/><category term="code"/><summary type="html"><![CDATA[It is a High-performance computing cluster of the University of Edinburgh]]></summary></entry><entry><title type="html">Understanding ChatGPT</title><link href="https://demi-wlw.github.io/blog/2023/ChatGPT/" rel="alternate" type="text/html" title="Understanding ChatGPT"/><published>2023-03-19T00:00:00+00:00</published><updated>2023-03-19T00:00:00+00:00</updated><id>https://demi-wlw.github.io/blog/2023/ChatGPT</id><content type="html" xml:base="https://demi-wlw.github.io/blog/2023/ChatGPT/"><![CDATA[<p class="text-justify">OpenAI is back in the headlines with news that it is updating its viral ChatGPT with a new version called <a href="https://openai.com/research/gpt-4">GPT-4</a>. If ChatGPT is the car, then GPT-4 is the engine: a powerful general technology that can be shaped down to a number of different uses. GPT-3 and now 4 are actually the internet’s best-known language-processing AI models.</p> <h2 id="what-is-chatgpt">What is ChatGPT?</h2> <p class="text-justify"><a href="https://chat.openai.com/chat">ChatGPT</a> is a large language model developed by OpenAI, which is based on the GPT (Generative Pre-trained Transformer) architecture. It is designed to understand natural language, generate human-like responses, and complete a variety of language tasks including language translation, text summarization, question-answering and more. It has been trained on a massive amount of text data from the internet, books, and other sources, allowing it to learn and understand a wide range of topics and provide informative and engaging responses.</p> <h2 id="what-is-gpt">What is GPT?</h2> <p class="text-justify">GPT (Generative Pre-trained Transformer) is a type of neural network architecture used for natural language processing (NLP) tasks, such as language modelling, text classification, and machine translation. It was first introduced by OpenAI in 2018 and has been used in a wide variety of applications.</p> <p class="text-justify">The GPT architecture is based on the <strong><em>Transformer</em></strong> model, which was introduced in 2017 by Vaswani et al.<d-cite key="attension2017"></d-cite> allowing to capture of long-term dependencies in text without the need for recurrent neural networks and generate high-quality language output. GPT models are <strong><em>pre-trained</em></strong> on large amounts of text data using an unsupervised learning approach, where the model is trained to <strong><em>predict the next word</em></strong> in a sequence of text. The pre-training allows the model to learn the statistical language patterns and contextual relationships within natural language text, which can then be fine-tuned on specific NLP tasks, such as language translation, text classification, and text generation with smaller amounts of labelled data.</p> <p class="text-justify">GPT has been a major breakthrough in natural language processing and the version GPT-3 has 175 billion parameters. It has been used to achieve state-of-the-art results on a wide variety of language tasks and has been adopted by a variety of industries for language-related applications such as chatbots, text summarization, and language translation. Its ability to pre-train large amounts of data and adapt to new tasks with few examples makes it a highly versatile and valuable tool for language-related applications.</p> <hr/> <h2 id="gpt-models">GPT Models</h2> <ul class="text-justify"> <li>GPT-1 is available <a href="https://openai.com/research/language-unsupervised">here</a> with paper “Improving Language Understanding by Generative Pre-Training”.</li> <li>GPT-2 is open-source <a href="https://openai.com/research/better-language-models">here</a> with paper “Language Models are Unsupervised Multitask Learners”.</li> <li>GPT-3 has published paper <d-cite key="GPT-3"></d-cite>.</li> <li>There are several open-source GPT alternatives such as <a href="https://github.com/facebookresearch/llama">LLaMA</a> by Meta AI with paper <d-cite key="touvron2023llama"></d-cite>.</li> <li>GPT-4 is not open-source but its general introduction is <a href="https://openai.com/product/gpt-4">here</a>.</li> </ul> <blockquote class="text-justify"> <p>The first version of GPT was introduced based on the ideas of transformers<d-footnote>A good website for detailed learning can be found <a href="https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro">here</a> <d-cite key="voita2020nlpCourse"></d-cite>.</d-footnote> and <a href="https://arxiv.org/abs/1511.01432">unsupervised pre-training</a>. Its results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well. In summary, GPT as the name suggested, adopts <strong><em>generative pre-training</em></strong> of a language model on a diverse corpus of unlabeled text, followed by discriminative <strong><em>supervised fine-tuning</em></strong> on each specific task.</p> </blockquote> <h3 id="generative-pre-training">Generative pre-training</h3> <p class="text-justify">The term <em>generative pre-training</em> represents the unsupervised pre-training of the generative model.<d-footnote>They used a multi-layer Transformer decoder to produce an output distribution over target tokens.</d-footnote> Given an unsupervised corpus of tokens $\mathcal{U} = (u_1,\dots,u_n)$, they use a standard language modelling objective to maximize the following likelihood:</p> \[L_1(\mathcal{U})=\sum_i\log P(u_i\mid u_{i-k},\dots,u_{i-1};\Theta)\] <p class="text-justify">where $k$ is the size of the context window, and the conditional probability $P$ is modelled using a neural network with parameters $\Theta$ trained using stochastic gradient descent. <strong>Intuitively, we train the Transformer-based model to predict the next token within the $k$-context window using unlabeled text from which we also extract the latent features $h$.</strong></p> <h3 id="supervised-fine-tuning">Supervised fine-tuning</h3> <p class="text-justify">After training the model with the objective function above, they adapt the parameters to the supervised target task which refers to supervised fine-tuning. Assume a labelled dataset $\mathcal{C}$, where each instance consists of a sequence of input tokens, $x^1,\dots, x^m$, along with a label $y$. The inputs are passed through the pre-trained model to obtain the final transformer block’s activation $h_l^m$, which is then fed into an added linear output layer with parameters $W_y$ to predict $y$:</p> \[P(y\mid x^1,\dots,x^m)=softmax(h_l^mW_y).\] <p>This gives us the following objective to maximize:</p> \[L_2(\mathcal{C})=\sum_{(x,y)}\log P(y\mid x^1,\dots,x^m)\] <p class="text-justify">They additionally found that including language modelling as an auxiliary objective to the fine-tuning helped learning by (a) improving the generalization of the supervised model, and (b) accelerating convergence. Specifically, we optimize the following objective (with weight $\lambda$): $L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda*L_1(\mathcal{C})$.</p> <p class="text-justify">Some tasks, like question answering or textual entailment, have structured inputs such as ordered sentence pairs, or triplets of document, questions, and answers that are different from the contiguous sequences of text inputs of the pre-trained model so they require some modifications to apply GPT. This results in the <strong><em>input transformations</em></strong> which allow us to avoid making extensive changes to the architecture across tasks. A brief description of these input transformations is shown in Figure 1 (credit to the <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">paper</a>).</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GPT1_inputTrans-480.webp 480w,/assets/img/GPT1_inputTrans-800.webp 800w,/assets/img/GPT1_inputTrans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/GPT1_inputTrans.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GPT-1 with input transformations" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote class="text-justify"> <p>GPT-3 was applied with tasks and <strong><em>few-shot</em></strong> demonstrations specified purely via text interaction with the model. Since fine-tuning involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task, it typically requires thousands to hundreds of thousands of labelled examples. However, the main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution, and the potential to exploit spurious features of the training data, potentially resulting in an unfair comparison with human performance.</p> </blockquote> <h3 id="few-shot-learning">Few-shot learning</h3> <p class="text-justify">Few-Shot is the term referring to the setting where the model is given a few demonstrations of the task at inference time as conditioning, but no weight updates are allowed. <em>Few-shot learning</em> involves learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. The primary goal in traditional Few-Shot frameworks is to learn a similarity function that can map the similarities between the classes in the support and query sets.</p> <p class="text-justify">Figure 2.1 <d-cite key="GPT-3"></d-cite> illustrates different settings, from which we see for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.</p> <p class="text-justify">The main advantages of few-shot are a major reduction in the need for task-specific data and a reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models. Also, a small amount of task-specific data is still required.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GPT3_fewShot-480.webp 480w,/assets/img/GPT3_fewShot-800.webp 800w,/assets/img/GPT3_fewShot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/GPT3_fewShot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GPT-3 Few-Shot learning" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="improvements-of-gpts">Improvements of GPTs</h2> <p class="text-justify"><strong>GPT-1</strong> employs the idea of unsupervised learning for training representations of words using large amounts of unlabeled data consisting of terabytes of information and then integrates supervised learning for fine-tuning to improve performance on a wide range of NLP tasks. However, it has drawbacks including (1) Compute requirements (expensive pre-training step) (2) The limits and bias of learning about the world through text, and (3) Still brittle generalization.</p> <p class="text-justify"><strong>GPT-2</strong> is a larger model with 1.5 billion parameters following the details of GPT-1 (117 million parameters) with a few modifications<d-footnote>These include pre-normalization, modified initialization, expanded vocabulary to 50,257, larger context size to 1024 tokens and larger batch-size of 512.</d-footnote>. This larger size allows it to capture more complex language patterns and relationships. In short, GPT-2 is a direct scale-up of GPT-1, with more than $10\times$ the parameters and trained on more than $10\times$ the amount of data.</p> <p><strong>GPT-3</strong> uses a variety of techniques to improve performance, including:</p> <ol class="text-justify"> <li>Larger size: has 175 billion parameters which allows to capture of even more complex language patterns and relationships.</li> <li>Adaptive computation: dynamically adjusts the number of parameters used for each task, allowing it to allocate more resources to complex tasks and fewer resources to simpler tasks.</li> <li>Few-shot learning: learns to perform a new task with just a few examples, making it highly flexible and adaptable to new tasks and contexts.</li> <li>Prompt engineering: can be given a natural language prompt to generate text that fits a specific context or follows a specific style.</li> </ol> <h2 id="limitations-of-generative-ai">Limitations of Generative AI</h2> <ul> <li>Generative AI tools are language machines rather than databases of knowledge – they work by predicting the next plausible word or section of programming code from patterns that have been ‘learnt’ from large data sets;</li> <li>AI tools have no understanding of what they generate. A knowledgeable human must check the work (often in iterations);</li> <li>The data sets that such tools are learning from are flawed and contain inaccuracies, biases and limitations;</li> <li>They generate text that is not always factually correct;</li> <li>They can create software/code that has security flaws, bugs, and use illegal libraries or calls – or infringe copyrights;</li> <li>Often the code or calculation produced by AI will look plausible but contain errors in detailed working on closer inspection. A human trained in that programming language should fully check any code or calculation produced in this way;</li> <li>The data their models are trained on is not up-to-date – they currently have limited or constrained data on the world and events after a certain point (2021 in the case of ChatGPT);</li> <li>They can generate offensive content;</li> <li>They produce fake citations and references;</li> <li>Such systems are amoral - they don’t know that it is wrong to generate offensive, inaccurate or misleading content;</li> <li>They include hidden plagiarism – meaning that they make use of words and ideas from human authors without referencing them, which we would consider as plagiarism;</li> <li>There are risks of copyright infringements on pictures and other copyrighted material.</li> </ul> <h4 id="reference-the-university-of-edinburgh">Reference: <a href="https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct/what-is-academic-misconduct">The University of Edinburgh</a></h4>]]></content><author><name>Luwei Wang</name></author><category term="site-posts"/><category term="machine-learning/AI"/><category term="NLP"/><category term="application"/><summary type="html"><![CDATA[A summary of ChatGPT and GPT techniques]]></summary></entry><entry><title type="html">Summary of Latex code for symbols</title><link href="https://demi-wlw.github.io/blog/2023/Latex-symbols/" rel="alternate" type="text/html" title="Summary of Latex code for symbols"/><published>2023-03-10T00:00:00+00:00</published><updated>2023-03-10T00:00:00+00:00</updated><id>https://demi-wlw.github.io/blog/2023/Latex-symbols</id><content type="html" xml:base="https://demi-wlw.github.io/blog/2023/Latex-symbols/"><![CDATA[]]></content><author><name></name></author><category term="site-posts"/><category term="code"/><summary type="html"><![CDATA[Redirecting to a pdf]]></summary></entry></feed>